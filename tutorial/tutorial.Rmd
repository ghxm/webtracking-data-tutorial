---
title: "Working with Web Tracking Data - Tutorial"
author:
  - "Felix Schmidt"
  - "Maximilian Haag"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: cosmo
---

# Setup and Prerequisites


For this tutorial to work properly, you must open the **project from the repository root folder** in RStudio or your IDE.

- ✓ Correct: Open the project from `/path/to/webtracking-data-tutorial/`
- ✗ Incorrect: Opening files individually or from subdirectories

This ensures all relative paths (to data files, helper scripts, etc.) work correctly.


**Before running the code below, make sure you have:**

1. Opened the project from the repository root folder (see above)
2. Downloaded the Web Tracking data (Campusfile) from [ZA5670](https://search.gesis.org/research_data/ZA5670)
3. Placed the data files in the `data/` folder:
   - `ZA5670_webtracking_data.csv`
   - `ZA5670_survey_data.csv`

## Install and Load Packages

Run this chunk to install (if needed) and load all required packages. This may take a few minutes on first run.

```{r install-packages, message=TRUE}
# Required packages
required_packages <- c(
  "tidyverse",    # Data manipulation and visualization
  "webtrackR",    # Web tracking data functions
  "adaR",         # URL parsing
  "here"          # Path management
)

# Install missing packages
new_packages <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
if(length(new_packages) > 0) {
  cat("Installing:", paste(new_packages, collapse = ", "), "\n")
  install.packages(new_packages, repos = "https://cloud.r-project.org/")
}

# Load all packages
cat("Loading packages...\n")
suppressPackageStartupMessages({
  library(tidyverse)
  library(webtrackR)
  library(adaR)
  library(here)
})

cat("✓ All packages loaded\n\n")
```

## Setup Data

The GESIS Web Tracking data contains only host-level information. We'll create an enhanced dataset with synthetic full URLs to practice URL parsing and classification. This takes 2-3 minutes on the first run.

```{r setup, include=TRUE, message=TRUE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Check R version (minimum 4.0.0)
if (getRversion() < "4.0.0") {
  warning("This tutorial requires R >= 4.0.0. You have ", getRversion())
}

# Load data functions and create enhanced dataset
# Use here::here() to find load_data.R from project root (works when knitting)
source(here::here("load_data.R"))

cat("✓ Setup complete!\n\n")
```




# Introduction

In this code along, we will work with the demo Web Tracking data from the GESIS panel.dbd (Campusfile: [ZA5670](https://search.gesis.org/research_data/ZA5670)). Note that the data has been augmented with synthetic full-URLs for demonstration purposes. The original dataset contains only host level visit information. The enhanced dataset includes synthetic full URLs for all entries to allow us to practice URL parsing and classification.
We will cover data loading, cleaning, and basic analysis using R. This tutorial focuses on **preprocessing web tracking data**, i.e., the essential steps you need before any analysis. We'll learn how to parse URLs, clean messy data, classify visits, and aggregate browsing behavior.

**What we'll cover:**

- Loading and understanding web tracking data structure
- Parsing URL components (domain, path, query parameters)
- Cleaning and deduplicating consecutive visits
- Classifying visits into meaningful categories
- Aggregating visit statistics by panelist
- Linking web tracking data with survey responses

**What we won't cover**:

- Session detection and boundaries
- Statistical modeling (regression, etc.)
- Advanced sequence analysis

Make sure you have downloaded the data and placed it in the `data/` folder as per the setup instructions.


# Working with web tracking data

## Loading and understanding the dataset

We'll take a look at the enhanced web tracking dataset and explore its structure.


```{r examine-data}
# Examine data structure
cat("=== Dataset Structure ===\n")
glimpse(wt_data)

cat("\n=== First 10 rows ===\n")
head(wt_data, 10)


```

We'll also check what time range our data is covering:

```{r check-time-range}
# Check time range
# Parse timestamps (ISO format with timezone)
wt_data <- wt_data %>%
  mutate(timestamp = as.POSIXct(start_time, format = "%Y-%m-%d %H:%M:%S"))

cat("\n=== Time Range ===\n")
cat("Start:", format(min(wt_data$timestamp, na.rm = TRUE), "%Y-%m-%d %H:%M"), "\n")
cat("End:", format(max(wt_data$timestamp, na.rm = TRUE), "%Y-%m-%d %H:%M"), "\n")
cat("Duration:",
    as.numeric(difftime(max(wt_data$timestamp, na.rm = TRUE),
                        min(wt_data$timestamp, na.rm = TRUE),
                        units = "days")),
    "days\n")


```

Now let's examine our panelists and visits in more detail. We'll count the total number of visits, unique panelists, and look at the distribution of visits per panelist. We'll also check the time range of the data to understand how long the tracking period is. Finally, we'll look at the distribution of visits per day per panelist to see if there are any temporal patterns in the data.

```{r}

# Count unique panelists
n_panelists <- n_distinct(wt_data$panelist_id)
cat("\n=== Basic Statistics ===\n")
cat("Total visits:", format(nrow(wt_data), big.mark = ","), "\n")
cat("Unique panelists:", n_panelists, "\n")

# Show visits per panelist (summary statistics)
cat("\n=== Visits per Panelist ===\n")
visits_per_panelist <- wt_data %>%
  count(panelist_id, name = "visits") %>%
  pull(visits)

cat("Mean:", round(mean(visits_per_panelist), 1), "\n")
cat("Median:", median(visits_per_panelist), "\n")
cat("Min:", min(visits_per_panelist), "\n")
cat("Max:", format(max(visits_per_panelist), big.mark = ","), "\n")

# Show visits per day per panelist
wt_data <- wt_data %>%
  mutate(date = as.Date(timestamp))

cat("\n=== Visits per Day per Panelist ===\n")
visits_per_day <- wt_data %>%
  group_by(panelist_id, date) %>%
  summarise(visits = n(), .groups = "drop") %>%
  pull(visits)

cat("Mean:", round(mean(visits_per_day), 1), "\n")
cat("Median:", median(visits_per_day), "\n")
cat("Max:", max(visits_per_day), "\n")


```



Of course, we can also visualize this data to get a better sense of the distribution of visits and temporal patterns:


```{r visualize-data-visits-per-user, fig.width=8, fig.height=5}
# Distribution of visits per panelist
visits_per_user <- wt_data %>%
  count(panelist_id, name = "visits")

ggplot(visits_per_user, aes(x = visits)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "white") +
  scale_x_continuous(labels = function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    title = "Distribution of Visits per Panelist",
    x = "Number of Visits",
    y = "Number of Panelists"
  ) +
  theme_minimal()
```




```{r visualize-data-visits-over-time, fig.width=10, fig.height=5}
# Visits over time (daily counts)
visits_over_time <- wt_data %>%
  count(date, name = "visits")

ggplot(visits_over_time, aes(x = date, y = visits)) +
  geom_line(color = "steelblue", linewidth = 0.8) +
  geom_smooth(method = "loess", se = TRUE, color = "darkred", alpha = 0.2) +
  scale_y_continuous(labels = function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    title = "Web Tracking Visits Over Time",
    x = "Date",
    y = "Number of Visits"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r visualize-data-visits-day-time, fig.width=10, fig.height=5}
# Visits by hour of day
visits_by_hour <- wt_data %>%
  mutate(hour = as.numeric(format(timestamp, "%H"))) %>%
  count(hour, name = "visits")

ggplot(visits_by_hour, aes(x = hour, y = visits)) +
  geom_col(fill = "steelblue") +
  scale_x_continuous(breaks = 0:23) +
  scale_y_continuous(labels = function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    title = "Web Tracking Activity by Hour of Day",
    x = "Hour of Day (0-23)",
    y = "Number of Visits"
  ) +
  theme_minimal()
```



The data contains not only full URLs but also entries that are just domains or privacy placeholders. This is common in web tracking data due to privacy settings and browser behavior. Understanding this heterogeneity is crucial for preprocessing and analysis. Let's categorize the URLs into types and see how many visits fall into each category. This will help us understand the composition of our dataset and inform our parsing and classification strategies. We'll also show sample URLs from each type to illustrate the differences.

```{r annotate-url-type}

# Annotate URL type (3 categories)
wt_data <- wt_data %>%
  mutate(
    url_type = case_when(
      url %in% c("blacked_out", "full_deny") ~ "Privacy placeholder",
      str_detect(url, "^https?://") | str_detect(url, "/") ~ "Full URL",
      TRUE ~ "Domain only"
    )
  )

# Show URL type distribution
cat("=== URL Type Distribution ===\n")
wt_data %>%
  count(url_type, sort = TRUE) %>%
  mutate(
    percentage = paste0(round(n / sum(n) * 100, 1), "%")
  ) %>%
  print()

# Show sample URLs from each type
cat("\n=== Sample URLs by Type ===\n")
wt_data %>%
  group_by(url_type) %>%
  slice_sample(n = 3) %>%
  select(url_type, url) %>%
  ungroup() %>%
  arrange(url_type) %>%
  print(n = 15)
```



## URL anatomy and parsing

URLs contain rich information encoded in their structure. We'll break them down into components (protocol, host, domain, path, query parameters) and learn how to extract each piece. We'll show both manual tidyverse approaches and webtrackR's built-in functions.

Because the data contains only the hostname / domain for some visits, we'll need to add `http://` to parse the URLs properly. This is a common issue with web tracking data where some entries are just domains without protocol or path for privacy reasons. Therefore, we want to add `http://` to all entries in the `url` column to ensure consistent parsing. This way, we can use URL parsing functions that expect a full URL format.

```{r add-protocol}

# TODO: Add http:// to all entries (that don't have a protocol) in the url column for consistent parsing



```

Now, we can begin dissecting the URLs into their components. While we could use manual string manipulation with regex and tidyverse functions, we'll primarily rely on webtrackR's built-in URL parsing functions for robustness and ease of use. These functions can handle edge cases and inconsistencies in the data more effectively than manual approaches.


![URL anatomy. Source: https://journals.sagepub.com/doi/10.1177/08944393241227868#fig2-08944393241227868](../assets/url_anatomy.jpg)



```{r parse-urls}
# TODO: Manual approach using tidyr::separate() and stringr::str_extract()
# Show basic regex patterns for extracting URL components

# TODO: Package approach using webtrackR::extract_host(), extract_domain(), extract_path()
# This is the main approach we'll use going forward

```

You can also do this in one step with the `adaR` package:

```{r ada-parse}

# TODO: Show how to use 
# TODO: Mention adaR::ada_parse() as alternative for robust URL parsing
# (don't demo extensively, just point out it exists)

# TODO: Compare results - show sample of parsed URLs to extract all URL components 

```





## Data cleaning and deduplication

**Description:** Web browsers and tracking systems create artifacts - consecutive duplicate visits, extremely short visits, invalid timestamps. We'll clean these artifacts to get a realistic picture of browsing behavior. We'll calculate visit durations and remove consecutive duplicates.

**Learning Objectives:**

- Understand why deduplication matters (browser artifacts, tracking quirks)
- Calculate visit duration from timestamp differences
- Remove consecutive duplicate visits within a time window
- Handle edge cases (last visit in sequence, missing timestamps)
- Understand trade-offs of different deduplication strategies

```{r clean-data}
# TODO: Manual approach using dplyr::lag(), filter(), time differences
# Show basic logic: if same URL within X seconds, remove duplicate

# TODO: Package approach using webtrackR::add_duration() and deduplicate()
# This is the main approach we'll use

# TODO: Compare before/after counts - how many visits were removed?
# TODO: Show distribution of visit durations
```

**Key Concepts:**

- **Consecutive duplicates**: Same URL visited multiple times in a row (often browser refreshes)
- **Visit duration**: Time between current visit and next visit
- **Deduplication window**: Time threshold for considering visits as duplicates (e.g., 1 second)

**Try it yourself:** How would you identify and remove visits with duration less than 1 second? How does changing the deduplication window affect your data?

---

## Basic URL classification

**Description:** Raw URLs are hard to analyze. We'll group them into meaningful categories like news, social media, e-commerce, search, and other. We'll use both simple pattern matching (regex) and webtrackR's classification functions.

**Learning Objectives:**

- Understand domain-based vs path-based classification
- Use regex patterns to detect categories (e.g., `/politik/` for news)
- Build simple classification rules with case_when()
- Use webtrackR's classify_visits() with classification lists
- Understand trade-offs of classification granularity

```{r classify-urls}
# TODO: Manual approach using case_when() with str_detect() patterns
# Simple categories: news, social, ecommerce, search, other
# Show examples of each category

# TODO: Package approach using webtrackR::classify_visits()
# Use simple classification list (5-7 categories max)

# TODO: Check classification results - count visits by category
# TODO: Show sample URLs from each category to verify
```

**Classification Examples:**

- **News**: Domains like spiegel.de, zeit.de, tagesschau.de; paths like `/politik/`, `/wirtschaft/`
- **Social/Video**: youtube.com, facebook.com, instagram.com
- **E-commerce**: amazon.de, ebay.de; paths like `/s?k=`, `/dp/`
- **Search**: google.com, bing.com; paths like `/search?q=`
- **Wikipedia**: wikipedia.org

**Try it yourself:** Can you further subdivide news into subcategories (politics, sports, culture) based on path patterns?

---

## Aggregation and summary statistics

**Description:** Once data is cleaned and classified, we can aggregate it to create meaningful summaries. We'll count visits, sum durations, and calculate statistics by panelist and category. These aggregated metrics form the basis for analysis.

**Learning Objectives:**

- Calculate per-panelist metrics (total visits, total duration)
- Aggregate by category (visits per category per person)
- Identify temporal patterns (visits by day, hour)
- Create summary tables suitable for analysis
- Understand wide vs long data formats

```{r aggregate-data}
# TODO: Manual approach using group_by(), summarize(), count()
# Count visits per panelist
# Sum duration per panelist per category
# Calculate visits by day/hour

# TODO: Package approach using webtrackR::sum_visits(), sum_durations()

# TODO: Create wide-format dataset (one row per panelist, columns for each category)
# This format is useful for regression analysis

# TODO: Show distribution of browsing activity across panelists
```

**Key Metrics:**

- **Total visits**: Count of all visits per panelist
- **Total duration**: Sum of all visit durations per panelist
- **Category counts**: Number of visits to each category per panelist
- **Temporal patterns**: Visits by day of week, hour of day

**Try it yourself:** Can you identify "heavy users" (top 10% most active panelists)? How does browsing behavior differ between weekdays and weekends?

---

## Quick survey integration

**Description:** Web tracking data becomes most powerful when linked with survey responses. We'll perform a simple join to add panelist demographics and show how this enables analysis of browsing patterns by user characteristics.

**Learning Objectives:**

- Understand one-to-many relationships (one survey response, many visits)
- Join web tracking data with survey data by panelist_id
- Check merge success (all panelists matched?)
- Prepare combined dataset for analysis

```{r join-survey}
# TODO: Load survey data
# TODO: Perform left_join() by panelist_id
# TODO: Check merge - any unmatched panelists?
# TODO: Show sample of combined data
```

**Key Concepts:**

- **One-to-many join**: Each panelist has one survey response but many web visits
- **Left join**: Keep all web visits, add survey data where available
- **Merge validation**: Always check if all records matched as expected

**Try it yourself:** How would you calculate average visits by age group or education level using the merged data?

---

## Wrap-up and next steps

**Description:** We've covered the core preprocessing pipeline for web tracking data. You now know how to load, parse, clean, classify, and aggregate browsing data. These skills form the foundation for any analysis of web tracking data.

**What we accomplished:**

1. ✓ Loaded and understood web tracking data structure
2. ✓ Parsed URLs into meaningful components
3. ✓ Cleaned and deduplicated visit sequences
4. ✓ Classified visits into categories
5. ✓ Aggregated statistics by panelist and category
6. ✓ Linked tracking data with survey responses

**Next steps:**

- Explore **session detection** (grouping visits into browsing sessions)
- Try **sequence analysis** (analyzing browsing sequences and transitions)
- Build **statistical models** (regression, classification)
- Implement **custom classification schemes** for your research questions

**Quick visualization of our final dataset:**

```{r final-viz}
# TODO: Create simple bar chart of visits by category
# TODO: Show distribution of browsing activity across panelists
```

# Exercises

Try these exercises on your own to practice what you've learned:

1. **Custom classification**: Create a classification scheme for German news outlets (Spiegel vs Zeit vs Tagesschau, etc.)

2. **Temporal analysis**: Calculate average visit duration by hour of day. When are people browsing most actively?

3. **User segmentation**: Identify different browsing profiles (news readers, shoppers, video watchers, etc.) based on category proportions

4. **Path analysis**: For YouTube videos, extract the video ID from URLs and identify most-watched videos

5. **Survey correlation**: Test if browsing behavior (total visits, category preferences) correlates with demographic variables from survey data





# Further resources

- [webtrackR documentation](https://github.com/schochastics/webtrackR)
- [adaR documentation](https://github.com/gesistsa/adaR)
- ["Analyis of Web Browsing Data" Practical guide](https://bernhardclemm.com/browsing-data-code-guide/guide.html#defining-visit-duration)
- [Clemm et al. "Analysis of Web Browsing Data: A Guide"] (https://journals.sagepub.com/doi/10.1177/08944393241227868)
- [GESIS Guides to Digital Behavioral Data #23: Overview of Working with Web Tracking Data](https://doi.org/10.60762/ggdbd25023.1.0)

---

# Reproducibility Information

This tutorial was developed and tested with the following system and package versions:

```{r reproducibility, echo=FALSE}
cat("Notebook built on:", as.character(Sys.Date()), "\n\n")

cat("System Information:\n")
cat("  R version:", as.character(getRversion()), "\n")
cat("  Platform:", R.version$platform, "\n")
cat("  OS:", R.version$os, "\n\n")

cat("Package Versions:\n")
package_info <- sessionInfo()$otherPkgs
if (!is.null(package_info)) {
  for (pkg_name in names(package_info)) {
    pkg <- package_info[[pkg_name]]
    cat("  ", pkg_name, ": v", as.character(pkg$Version), "\n", sep = "")
  }
} else {
  cat("  No packages loaded in session\n")
}
```

<details>
<summary>Full Session Info (click to expand)</summary>

```{r session-info, echo=FALSE}
sessionInfo()
```

</details>

**Minimum Requirements:**

- R version: >= 4.0.0
- Required packages: tidyverse, webtrackR, adaR, here

